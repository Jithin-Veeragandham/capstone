{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 12                -1  1         0  models.common.MP                        []                            \n",
      " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
      " 25                -1  1         0  models.common.MP                        []                            \n",
      " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
      " 38                -1  1         0  models.common.MP                        []                            \n",
      " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
      " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
      " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
      " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 76                -1  1         0  models.common.MP                        []                            \n",
      " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
      " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 89                -1  1         0  models.common.MP                        []                            \n",
      " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
      " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
      " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
      "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
      "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
      "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
      "105   [102, 103, 104]  1    457725  models.yolo.Detect                      [80, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
      "Model Summary: 407 layers, 37620125 parameters, 37620125 gradients\n",
      "\n",
      "YOLOR  2023-5-30 torch 1.13.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2060 with Max-Q Design, 6143.6875MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-18 18:29:13.340\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-18 18:29:13.342\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from boxmot import DeepOCSORT\n",
    "\n",
    "model = torch.hub.load('C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\yolov7-main\\\\yolov7-main', 'custom', \"C:\\\\Users\\\\jithi\\\\Downloads\\\\yolov7.pt\",force_reload=True, source='local',trust_repo=True)\n",
    "tracker = DeepOCSORT(\n",
    "    model_weights=Path('osnet_x0_25_msmt17.pt'),\n",
    "    device='cuda:0',\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:115: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"person\"], label=\"person\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:116: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"cat\"], label=\"cat\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:117: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"car\"], label=\"car\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:118: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"truck\"], label=\"truck\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:119: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"bicycle\"], label=\"bicycle\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:120: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"motorcycle\"], label=\"motorcycle\"),\n",
      "C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_19512\\1359734425.py:121: GradioUnusedKwargWarning: You have unused kwarg parameters in Checkbox, please remove them: {'initial_value': False}\n",
      "  gr.Checkbox(initial_value=checkbox_states[\"dog\"], label=\"dog\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize checkbox states\n",
    "checkbox_states = {\"person\": False, \"cat\": False, \"car\": False, \"truck\": False,\"bicycle\":False,\"motorcycle\":False,\"dog\":False}\n",
    "\n",
    "def update_checkboxes_and_process_video(person, cat, car, truck, bicycle, motorcycle, dog, video_path):\n",
    "    # Update checkbox states\n",
    "    checkbox_states[\"person\"] = person\n",
    "    checkbox_states[\"cat\"] = cat\n",
    "    checkbox_states[\"car\"] = car\n",
    "    checkbox_states[\"truck\"] = truck\n",
    "    checkbox_states[\"bicycle\"] = bicycle\n",
    "    checkbox_states[\"motorcycle\"] = motorcycle\n",
    "    checkbox_states[\"dog\"] = dog\n",
    "    checkbox_states[\"path\"] = video_path\n",
    "    # Write to JSON file\n",
    "    with open(\"checkbox_states.json\", \"w\") as f:\n",
    "        json.dump(checkbox_states, f)\n",
    "\n",
    "    # Process Video\n",
    "    if video_path:\n",
    "        process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
    "\n",
    "    return f\"Updated and saved checkbox states: {checkbox_states}\"\n",
    "\n",
    "def process_video(video_path):\n",
    "    # video_path=\"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\track.mp4\"\n",
    "    json_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\checkbox_states.json\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "    video_path=checkbox_states.get(\"path\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    csv_file = open('logs.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Time', 'ID', 'Class'])\n",
    "\n",
    "    # Define colors for each class\n",
    "    class_colors = {\n",
    "        0: (0, 0, 255),\n",
    "        2: (0, 255, 0),\n",
    "        16: (255, 0, 0),\n",
    "        # Add more classes and their colors\n",
    "    }\n",
    "    id_to_name = {\n",
    "    0: 'person',\n",
    "    2: 'car',\n",
    "    16: 'dog'\n",
    "    # 16:'none',\n",
    "    # 7:'bird',\n",
    "    # 58:'pot',\n",
    "    # 10:'none'\n",
    "    }\n",
    "\n",
    "    thickness = 2\n",
    "    fontscale = 0.5\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "        results = model(frame)\n",
    "        df = results.pandas().xyxy[0]\n",
    "        detections = []\n",
    "\n",
    "            # Check if this class should have a bounding box drawn, according to the JSON file\n",
    "        for _, row in df.iterrows():\n",
    "            class_name = row['class']\n",
    "            class_str=id_to_name.get(class_name)\n",
    "            \n",
    "            # x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "            # class_name = row['class']\n",
    "            # detections.append([x1, y1, x2, y2, 1.0, class_name])\n",
    "                \n",
    "                \n",
    "            if checkbox_states.get(str(class_str), False):\n",
    "                x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "                class_name = row['class']\n",
    "                detections.append([x1, y1, x2, y2, 1.0, class_name])\n",
    "\n",
    "        tracks = tracker.update(np.array(detections), frame)\n",
    "\n",
    "        xyxys = tracks[:, 0:4].astype('int')\n",
    "        ids = tracks[:, 4].astype('int')\n",
    "        confs = tracks[:, 5]\n",
    "        clss = tracks[:, 6].astype('int')\n",
    "        inds = tracks[:, 7].astype('int')\n",
    "\n",
    "        if tracks.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "                color = class_colors.get(cls, (0, 0, 0))  # Default color is black\n",
    "                frame = cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, thickness)\n",
    "                text = f'id: {id}, class: {id_to_name.get(cls)}'\n",
    "                frame = cv2.putText(frame, text, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, fontscale, color, thickness)\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                csv_writer.writerow([current_time, id, cls])    \n",
    "\n",
    "        cv2.imshow('Video with Object IDs and Classes', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Gradio interface setup\n",
    "demo = gr.Interface(\n",
    "    update_checkboxes_and_process_video,\n",
    "    [\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"person\"], label=\"person\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"cat\"], label=\"cat\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"car\"], label=\"car\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"truck\"], label=\"truck\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"bicycle\"], label=\"bicycle\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"motorcycle\"], label=\"motorcycle\"),\n",
    "        gr.Checkbox(initial_value=checkbox_states[\"dog\"], label=\"dog\"),\n",
    "        gr.Textbox(label=\"Video Path\"),\n",
    "    ],\n",
    "    \"text\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "id_to_name = {\n",
    "    0: 'person',\n",
    "    2: 'car',\n",
    "    16: 'dog'\n",
    "    # 16:'none',\n",
    "    # 7:'bird',\n",
    "    # 58:'pot',\n",
    "    # 10:'none'\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize checkbox states\n",
    "checkbox_states = {\"person\": False, \"cat\": False, \"car\": False, \"truck\": False,\"bicycle\":False,\"motorcycle\":False,\"dog\":False}\n",
    "\n",
    "def create_entry_exit_logs():\n",
    "    file_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\logs.csv\"\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the 'Time' column to datetime format for accurate processing\n",
    "    df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Group by ID and get the first and last time for each ID\n",
    "    result_df = df.groupby('ID').agg(\n",
    "        Class=('Class', 'first'),\n",
    "        Entry_Time=('Time', 'first'),\n",
    "        Exit_Time=('Time', 'last')\n",
    "    ).reset_index()    \n",
    "    result_df['Class'] = result_df['Class'].map(id_to_name)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def update_checkboxes_and_process_video(person, cat, car, truck, bicycle, motorcycle, dog, video_path):\n",
    "    # Update checkbox states\n",
    "    checkbox_states[\"person\"] = person\n",
    "    checkbox_states[\"cat\"] = cat\n",
    "    checkbox_states[\"car\"] = car\n",
    "    checkbox_states[\"truck\"] = truck\n",
    "    checkbox_states[\"bicycle\"] = bicycle\n",
    "    checkbox_states[\"motorcycle\"] = motorcycle\n",
    "    checkbox_states[\"dog\"] = dog\n",
    "    checkbox_states[\"path\"] = video_path\n",
    "    # Write to JSON file\n",
    "    with open(\"checkbox_states.json\", \"w\") as f:\n",
    "        json.dump(checkbox_states, f)\n",
    "\n",
    "    # Process Video\n",
    "    if video_path:\n",
    "        process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
    "\n",
    "    return f\"Updated and saved checkbox states: {checkbox_states}\"\n",
    "\n",
    "def process_video(video_path):\n",
    "    # video_path=\"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\track.mp4\"\n",
    "    json_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\checkbox_states.json\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "    video_path=checkbox_states.get(\"path\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    csv_file = open('logs.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Time', 'ID', 'Class'])\n",
    "\n",
    "    # Define colors for each class\n",
    "    class_colors = {\n",
    "        0: (0, 0, 255),\n",
    "        2: (0, 255, 0),\n",
    "        16: (255, 0, 0),\n",
    "        # Add more classes and their colors\n",
    "    }\n",
    "    # id_to_name = {\n",
    "    # 0: 'person',\n",
    "    # 2: 'car',\n",
    "    # 16: 'dog'\n",
    "    # # 16:'none',\n",
    "    # # 7:'bird',\n",
    "    # # 58:'pot',\n",
    "    # # 10:'none'\n",
    "    # }\n",
    "\n",
    "    thickness = 2\n",
    "    fontscale = 0.5\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "        results = model(frame)\n",
    "        df = results.pandas().xyxy[0]\n",
    "        detections = []\n",
    "\n",
    "            # Check if this class should have a bounding box drawn, according to the JSON file\n",
    "        for _, row in df.iterrows():\n",
    "            class_name = row['class']\n",
    "            class_str=id_to_name.get(class_name)\n",
    "\n",
    "            if checkbox_states.get(str(class_str), False):\n",
    "                x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "                class_name = row['class']\n",
    "                detections.append([x1, y1, x2, y2, 1.0, class_name])\n",
    "\n",
    "        tracks = tracker.update(np.array(detections), frame)\n",
    "\n",
    "        xyxys = tracks[:, 0:4].astype('int')\n",
    "        ids = tracks[:, 4].astype('int')\n",
    "        confs = tracks[:, 5]\n",
    "        clss = tracks[:, 6].astype('int')\n",
    "        inds = tracks[:, 7].astype('int')\n",
    "\n",
    "        if tracks.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "                color = class_colors.get(cls, (0, 0, 0))  # Default color is black\n",
    "                frame = cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, thickness)\n",
    "                text = f'id: {id}, class: {id_to_name.get(cls)}'\n",
    "                frame = cv2.putText(frame, text, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, fontscale, color, thickness)\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                csv_writer.writerow([current_time, id, cls])    \n",
    "\n",
    "        cv2.imshow('Video with Object IDs and Classes', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1 style='font-size: 40px;'>SentinelGuard</h1>\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Parameters\"):\n",
    "            with gr.Row():\n",
    "                checkbox_person = gr.Checkbox(label=\"person\", value=checkbox_states[\"person\"])\n",
    "                checkbox_cat = gr.Checkbox(label=\"cat\", value=checkbox_states[\"cat\"])\n",
    "                checkbox_car = gr.Checkbox(label=\"car\", value=checkbox_states[\"car\"])\n",
    "                checkbox_truck = gr.Checkbox(label=\"truck\", value=checkbox_states[\"truck\"])\n",
    "                checkbox_bicycle = gr.Checkbox(label=\"bicycle\", value=checkbox_states[\"bicycle\"])\n",
    "                checkbox_motorcycle = gr.Checkbox(label=\"motorcycle\", value=checkbox_states[\"motorcycle\"])\n",
    "                checkbox_dog = gr.Checkbox(label=\"dog\", value=checkbox_states[\"dog\"])\n",
    "            video_path_input = gr.Textbox(label=\"Video Path\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            submit_button.click(\n",
    "                update_checkboxes_and_process_video, \n",
    "                inputs=[checkbox_person, checkbox_cat, checkbox_car, checkbox_truck, checkbox_bicycle, checkbox_motorcycle, checkbox_dog, video_path_input], \n",
    "                outputs=[]\n",
    "            )\n",
    "        with gr.TabItem(\"Logs\"):\n",
    "            create_logs_button = gr.Button(\"Create Entry/Exit time logs\")\n",
    "            logs_output = gr.Dataframe()\n",
    "            create_logs_button.click(\n",
    "                create_entry_exit_logs,\n",
    "                outputs=logs_output\n",
    "            )\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\routes.py\", line 516, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\utils.py\", line 650, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 57, in update_checkboxes_and_process_video\n",
      "    process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 116, in process_video\n",
      "    tracks = tracker.update(np.array(detections), frame)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\boxmot\\trackers\\deepocsort\\deep_ocsort.py\", line 368, in update\n",
      "    assert len(dets.shape) == 2, \"Unsupported 'dets' dimensions, valid number of dimensions is two\"\n",
      "AssertionError: Unsupported 'dets' dimensions, valid number of dimensions is two\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\routes.py\", line 516, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\utils.py\", line 650, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 57, in update_checkboxes_and_process_video\n",
      "    process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 116, in process_video\n",
      "    tracks = tracker.update(np.array(detections), frame)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\boxmot\\trackers\\deepocsort\\deep_ocsort.py\", line 368, in update\n",
      "    assert len(dets.shape) == 2, \"Unsupported 'dets' dimensions, valid number of dimensions is two\"\n",
      "AssertionError: Unsupported 'dets' dimensions, valid number of dimensions is two\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\routes.py\", line 516, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\utils.py\", line 650, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 57, in update_checkboxes_and_process_video\n",
      "    process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 118, in process_video\n",
      "    xyxys = tracks[:, 0:4].astype('int')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\routes.py\", line 516, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\jithi\\.conda\\envs\\pytorch_OIIO\\lib\\site-packages\\gradio\\utils.py\", line 650, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 57, in update_checkboxes_and_process_video\n",
      "    process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
      "  File \"C:\\Users\\jithi\\AppData\\Local\\Temp\\ipykernel_28936\\1400217875.py\", line 118, in process_video\n",
      "    xyxys = tracks[:, 0:4].astype('int')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "id_to_name = {\n",
    "    0: 'person',\n",
    "    2: 'car',\n",
    "    16: 'dog'\n",
    "    # 16:'none',\n",
    "    # 7:'bird',\n",
    "    # 58:'pot',\n",
    "    # 10:'none'\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize checkbox states\n",
    "checkbox_states = {\"person\": False, \"cat\": False, \"car\": False, \"truck\": False,\"bicycle\":False,\"motorcycle\":False,\"dog\":False}\n",
    "\n",
    "def create_entry_exit_logs():\n",
    "    file_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\logs.csv\"\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the 'Time' column to datetime format for accurate processing\n",
    "    df['Time'] = pd.to_datetime(df['Time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Group by ID and get the first and last time for each ID\n",
    "    result_df = df.groupby('ID').agg(\n",
    "        Class=('Class', 'first'),\n",
    "        Entry_Time=('Time', 'first'),\n",
    "        Exit_Time=('Time', 'last'),\n",
    "        Vid_Timestamp=('vid_timestamp', 'first')  # Include Vid_Timestamp\n",
    "    ).reset_index()   \n",
    "    result_df['Class'] = result_df['Class'].map(id_to_name)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def update_checkboxes_and_process_video(person, cat, car, truck, bicycle, motorcycle, dog, video_path):\n",
    "    # Update checkbox states\n",
    "    checkbox_states[\"person\"] = person\n",
    "    checkbox_states[\"cat\"] = cat\n",
    "    checkbox_states[\"car\"] = car\n",
    "    checkbox_states[\"truck\"] = truck\n",
    "    checkbox_states[\"bicycle\"] = bicycle\n",
    "    checkbox_states[\"motorcycle\"] = motorcycle\n",
    "    checkbox_states[\"dog\"] = dog\n",
    "    checkbox_states[\"path\"] = video_path\n",
    "    # Write to JSON file\n",
    "    with open(\"checkbox_states.json\", \"w\") as f:\n",
    "        json.dump(checkbox_states, f)\n",
    "\n",
    "    # Process Video\n",
    "    if video_path:\n",
    "        process_video(video_path)  # Function to process the video, similar to your first code snippet\n",
    "\n",
    "    return f\"Updated and saved checkbox states: {checkbox_states}\"\n",
    "\n",
    "def process_video(video_path):\n",
    "    # video_path=\"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\track.mp4\"\n",
    "    json_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\capstone\\\\checkbox_states.json\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "    video_path=checkbox_states.get(\"path\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    csv_file = open('logs.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Time', 'ID', 'Class','vid_timestamp'])\n",
    "\n",
    "    # Define colors for each class\n",
    "    class_colors = {\n",
    "        0: (0, 0, 255),\n",
    "        2: (0, 255, 0),\n",
    "        16: (255, 0, 0),\n",
    "        # Add more classes and their colors\n",
    "    }\n",
    "    # id_to_name = {\n",
    "    # 0: 'person',\n",
    "    # 2: 'car',\n",
    "    # 16: 'dog'\n",
    "    # # 16:'none',\n",
    "    # # 7:'bird',\n",
    "    # # 58:'pot',\n",
    "    # # 10:'none'\n",
    "    # }\n",
    "\n",
    "    thickness = 2\n",
    "    fontscale = 0.5\n",
    "    current_frame = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        start_time = cv2.getTickCount()\n",
    "        with open(json_path, 'r') as f:\n",
    "            checkbox_states = json.load(f)\n",
    "        results = model(frame)\n",
    "        df = results.pandas().xyxy[0]\n",
    "        detections = []\n",
    "\n",
    "            # Check if this class should have a bounding box drawn, according to the JSON file\n",
    "        for _, row in df.iterrows():\n",
    "            class_name = row['class']\n",
    "            class_str=id_to_name.get(class_name)\n",
    "\n",
    "            if checkbox_states.get(str(class_str), False):\n",
    "                x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "                class_name = row['class']\n",
    "                detections.append([x1, y1, x2, y2, 1.0, class_name])\n",
    "\n",
    "        tracks = tracker.update(np.array(detections), frame)\n",
    "\n",
    "        xyxys = tracks[:, 0:4].astype('int')\n",
    "        ids = tracks[:, 4].astype('int')\n",
    "        confs = tracks[:, 5]\n",
    "        clss = tracks[:, 6].astype('int')\n",
    "        inds = tracks[:, 7].astype('int')\n",
    "        if tracks.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "                color = class_colors.get(cls, (0, 0, 0))  # Default color is black\n",
    "                frame = cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, thickness)\n",
    "                text = f'id: {id}, class: {id_to_name.get(cls)}'\n",
    "                frame = cv2.putText(frame, text, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, fontscale, color, thickness)\n",
    "                end_time = cv2.getTickCount()\n",
    "                processing_time = (end_time - start_time) / cv2.getTickFrequency()\n",
    "                vid_timestamp = current_frame / fps + processing_time\n",
    "\n",
    "                current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                csv_writer.writerow([current_time, id, cls,vid_timestamp])    \n",
    "                current_frame += 1\n",
    "\n",
    "        cv2.imshow('Video with Object IDs and Classes', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1 style='font-size: 40px;'>SentinelGuard</h1>\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Parameters\"):\n",
    "            with gr.Row():\n",
    "                checkbox_person = gr.Checkbox(label=\"person\", value=checkbox_states[\"person\"])\n",
    "                checkbox_cat = gr.Checkbox(label=\"cat\", value=checkbox_states[\"cat\"])\n",
    "                checkbox_car = gr.Checkbox(label=\"car\", value=checkbox_states[\"car\"])\n",
    "                checkbox_truck = gr.Checkbox(label=\"truck\", value=checkbox_states[\"truck\"])\n",
    "                checkbox_bicycle = gr.Checkbox(label=\"bicycle\", value=checkbox_states[\"bicycle\"])\n",
    "                checkbox_motorcycle = gr.Checkbox(label=\"motorcycle\", value=checkbox_states[\"motorcycle\"])\n",
    "                checkbox_dog = gr.Checkbox(label=\"dog\", value=checkbox_states[\"dog\"])\n",
    "            video_path_input = gr.Textbox(label=\"Video Path\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            submit_button.click(\n",
    "                update_checkboxes_and_process_video, \n",
    "                inputs=[checkbox_person, checkbox_cat, checkbox_car, checkbox_truck, checkbox_bicycle, checkbox_motorcycle, checkbox_dog, video_path_input], \n",
    "                outputs=[]\n",
    "            )\n",
    "        with gr.TabItem(\"Logs\"):\n",
    "            create_logs_button = gr.Button(\"Create Entry/Exit time logs\")\n",
    "            logs_output = gr.Dataframe()\n",
    "            create_logs_button.click(\n",
    "                create_entry_exit_logs,\n",
    "                outputs=logs_output\n",
    "            )\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_OIIO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
