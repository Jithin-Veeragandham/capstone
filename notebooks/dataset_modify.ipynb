{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "import fiftyone as fo\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\", \n",
    "    split=\"train\", \n",
    "    label_types=[\"detections\"], \n",
    "    classes=[\"person\", \"car\", \"truck\", \"bicycle\", \"motorcycle\", \"cat\", \"dog\"]\n",
    ")\n",
    "\n",
    "export_dir = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\"\n",
    "\n",
    "# Export dataset in YOLO format\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.YOLOv4Dataset,\n",
    "    label_field=\"ground_truth\",  # this should be the name of the detections field in your FiftyOne dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "name = \"coco_yolo\"\n",
    "\n",
    "# Check if a dataset with the given name already exists\n",
    "if name in fo.list_datasets():\n",
    "    # Delete the existing dataset\n",
    "    fo.delete_dataset(name)\n",
    "\n",
    "# The directory containing the dataset to import\n",
    "dataset_dir = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\"\n",
    "\n",
    "# The type of the dataset being imported\n",
    "dataset_type = fo.types.YOLOv4Dataset # for example\n",
    "\n",
    "# Import the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=dataset_type,\n",
    "    name=name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the obj.names file and data directory\n",
    "names_path = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\\\\obj.names\"\n",
    "data_dir = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\\\\data\"\n",
    "\n",
    "# Read obj.names and get mapping of class names to indices\n",
    "with open(names_path, 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "name_to_idx = {name: idx for idx, name in enumerate(names)}\n",
    "\n",
    "# Define the desired classes\n",
    "desired_classes = [\"person\", \"car\", \"truck\", \"bicycle\", \"motorcycle\", \"cat\", \"dog\"]\n",
    "desired_indices = [str(name_to_idx[name]) for name in desired_classes if name in name_to_idx]\n",
    "\n",
    "# Iterate over all .txt files in the data directory and filter annotations\n",
    "txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]\n",
    "for txt_file in tqdm(txt_files, desc=\"Processing annotations\"):\n",
    "    file_path = os.path.join(data_dir, txt_file)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    filtered_lines = [line for line in lines if line.split()[0] in desired_indices]\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(filtered_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# renaming annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mapping Original Classes to Indices:\n",
    "\n",
    "original_classes = [\n",
    "    \"umbrella\", \"person\", \"dog\", \"horse\", \"potted plant\", \"elephant\", \"car\",\n",
    "    \"truck\", \"stop sign\", \"clock\", \"train\", \"motorcycle\", \"bicycle\", \"skateboard\",\n",
    "    \"handbag\", \"bench\", \"chair\", \"fork\", \"knife\", \"pizza\", \"dining table\", \"cup\",\n",
    "    \"cake\", \"spoon\", \"book\", \"giraffe\", \"kite\", \"tie\", \"scissors\", \"baseball bat\",\n",
    "    \"snowboard\", \"bottle\", \"couch\", \"remote\", \"airplane\", \"traffic light\", \"backpack\",\n",
    "    \"bus\", \"suitcase\", \"microwave\", \"frisbee\", \"wine glass\", \"teddy bear\", \"cell phone\",\n",
    "    \"refrigerator\", \"oven\", \"baseball glove\", \"sports ball\", \"broccoli\", \"skis\", \"boat\",\n",
    "    \"tennis racket\", \"donut\", \"cat\", \"bird\", \"surfboard\", \"bed\", \"toothbrush\", \"vase\",\n",
    "    \"tv\", \"laptop\", \"mouse\", \"bowl\", \"sandwich\", \"hot dog\", \"parking meter\", \"fire hydrant\",\n",
    "    \"banana\", \"orange\", \"cow\", \"sink\", \"carrot\", \"sheep\", \"apple\", \"toilet\", \"keyboard\",\n",
    "    \"zebra\", \"hair drier\", \"bear\", \"toaster\"\n",
    "]\n",
    "\n",
    "# Dictionary that maps class names to their indices\n",
    "original_mapping = {cls: idx for idx, cls in enumerate(original_classes)}\n",
    "\n",
    "# 2. Mapping Selected Classes to New Indices:\n",
    "\n",
    "selected_classes = [\"person\", \"car\", \"truck\", \"bicycle\", \"motorcycle\", \"cat\", \"dog\"]\n",
    "new_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_directory = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\\\\data\"\n",
    "\n",
    "# Iterate through each file in the data directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    \n",
    "    if filepath.endswith(\".txt\"):\n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        updated_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_idx = int(parts[0])\n",
    "            class_name = original_classes[class_idx]\n",
    "\n",
    "            # If this class is one of our selected classes, update its index\n",
    "            if class_name in new_mapping:\n",
    "                new_idx = new_mapping[class_name]\n",
    "                updated_lines.append(f\"{new_idx} {' '.join(parts[1:])}\\n\")\n",
    "\n",
    "        # Save the updated lines back to the file\n",
    "        with open(filepath, \"w\") as file:\n",
    "            file.writelines(updated_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting into train test and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_img = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco_split\\\\images\\\\train\"\n",
    "train_path_label = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco_split\\\\labels\\\\train\"\n",
    "val_path_img = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco_split\\\\images\\\\val\"\n",
    "val_path_label = \"C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco_split\\\\labels\\\\val\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(path,neg_path=None, split = 0.2):\n",
    "\n",
    "    files = list(set([name[:-4] for name in os.listdir(path)])) ## removing duplicate names i.e. counting only number of images\n",
    "\n",
    "    print (f\"no of images:{len(files)} \")\n",
    "    random.seed(42)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    test_size = int(len(files) * split)\n",
    "    train_size = len(files) - test_size\n",
    "\n",
    "    # creating required directories\n",
    "\n",
    "    os.makedirs(train_path_img, exist_ok = True)\n",
    "    os.makedirs(train_path_label, exist_ok = True)\n",
    "    os.makedirs(val_path_img, exist_ok = True)\n",
    "    os.makedirs(val_path_label, exist_ok = True)\n",
    "\n",
    "\n",
    "    # copying images to train folder\n",
    "    for filex in tqdm(files[:train_size]):\n",
    "      if filex == 'classes':\n",
    "          continue\n",
    "      shutil.copy2(path + filex + '.jpg',f\"{train_path_img}/\" + filex + '.jpg' )\n",
    "      shutil.copy2(path + filex + '.txt', f\"{train_path_label}/\" + filex + '.txt')\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Training data created with 80% split {len(files[:train_size])} images \")\n",
    "\n",
    "    if neg_path:\n",
    "        neg_images = list(set([name[:-4] for name in os.listdir(neg_path)])) # removing duplicate names i.e. counting only number of images\n",
    "        for filex in tqdm(neg_images):\n",
    "            shutil.copy2(neg_path+filex+ \".jpg\", f\"{train_path_img}/\" + filex + '.jpg')\n",
    "\n",
    "        print(f\"Total  {len(neg_images)} negative images added to the training data\")\n",
    "\n",
    "        print(f\"TOTAL Training data created with {len(files[:train_size]) + len(neg_images)} images \")\n",
    "\n",
    "\n",
    "\n",
    "    for filex in tqdm(files[train_size:]):\n",
    "      if filex == 'classes':\n",
    "          continue\n",
    "      shutil.copy2(path + filex + '.jpg', f\"{val_path_img}/\" + filex + '.jpg' )\n",
    "      shutil.copy2(path + filex + '.txt', f\"{val_path_label}/\" + filex + '.txt')\n",
    "\n",
    "    print(f\"Testing data created with a total of {len(files[train_size:])} images \")\n",
    "\n",
    "train_test_split('C:\\\\Users\\\\jithi\\\\OneDrive\\\\Desktop\\\\VsCode\\\\coco\\\\data\\\\') "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
